<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">
    <meta name="color-scheme" content="light dark">

    
      <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests; block-all-mixed-content; default-src 'self'; child-src 'self'; font-src 'self' https://fonts.gstatic.com https://cdn.jsdelivr.net/ https://cdnjs.cloudflare.com; form-action 'self'; frame-src 'self'; img-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com/ https://cdn.jsdelivr.net/; script-src 'self' 'unsafe-inline' https://www.google-analytics.com https://cdn.jsdelivr.net https://cdnjs.cloudflare.com; prefetch-src 'self'; connect-src 'self' https://www.google-analytics.com;">

    

    
    <meta name="description" content="Let&rsquo;s assume that we have a stream of data points, and we want to detect outliers i.e. points that deviate from the norm. This is interesting because usually such data are linked to faulty or malicious behavior.
The fact that our data points are available as a stream means that we cannot &ldquo;look&rdquo; at them more than once. Therefore we need an algorithm with complexity linear to the number of points of the stream.">
    <meta name="keywords" content="">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Anomaly Detection on Data Streams in a few lines of Bash"/>
<meta name="twitter:description" content="Let&rsquo;s assume that we have a stream of data points, and we want to detect outliers i.e. points that deviate from the norm. This is interesting because usually such data are linked to faulty or malicious behavior.
The fact that our data points are available as a stream means that we cannot &ldquo;look&rdquo; at them more than once. Therefore we need an algorithm with complexity linear to the number of points of the stream."/>

    <meta property="og:title" content="Anomaly Detection on Data Streams in a few lines of Bash" />
<meta property="og:description" content="Let&rsquo;s assume that we have a stream of data points, and we want to detect outliers i.e. points that deviate from the norm. This is interesting because usually such data are linked to faulty or malicious behavior.
The fact that our data points are available as a stream means that we cannot &ldquo;look&rdquo; at them more than once. Therefore we need an algorithm with complexity linear to the number of points of the stream." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nikosg.com/anomaly-detection-on-data-streams-in-a-few-lines-of-bash/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2019-11-02T00:00:00+00:00" />
<meta property="article:modified_time" content="2019-11-02T00:00:00+00:00" />


    <title>
  Anomaly Detection on Data Streams in a few lines of Bash Â· Nikos Gavalas
</title>

    
      <link rel="canonical" href="https://nikosg.com/anomaly-detection-on-data-streams-in-a-few-lines-of-bash/">
    

    <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>

    
      
      
      <link rel="stylesheet" href="/css/coder.min.708686f8ab8176e91d44fcbe488a0fe0333b94f405cf18a52383d67ba22f0ccb.css" integrity="sha256-cIaG&#43;KuBdukdRPy&#43;SIoP4DM7lPQFzxilI4PWe6IvDMs=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="/css/coder-dark.min.aa883b9ce35a8ff4a2a5008619005175e842bb18a8a9f9cc2bbcf44dab2d91fa.css" integrity="sha256-qog7nONaj/SipQCGGQBRdehCuxioqfnMK7z0Tastkfo=" crossorigin="anonymous" media="screen" />
      
    

    
      <link rel="stylesheet" href="/css/custom.css" />
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.16.2/build/styles/default.min.css" />
    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    

    <meta name="generator" content="Hugo 0.111.3">
  </head>

  
  
    
  
  <body class="preload-transitions colorscheme-auto"
        onload=""
  >
    
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Nikos Gavalas
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Posts</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://nikosg.com/anomaly-detection-on-data-streams-in-a-few-lines-of-bash/">
              Anomaly Detection on Data Streams in a few lines of Bash
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2019-11-02T00:00:00Z'>
                November 2, 2019
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              7-minute read
            </span>
          </div>
          
          <div class="categories">
  <i class="fa fa-folder" aria-hidden="true"></i>
    <a href="/categories/algorithms/">algorithms</a></div>

          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/algorithms/">algorithms</a>
    </span>
      <span class="separator">â€¢</span>
    <span class="tag">
      <a href="/tags/programming/">programming</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <p>Let&rsquo;s assume that we have a stream of data points, and we want to detect outliers i.e. points that deviate from the norm. This is interesting because usually such data are linked to faulty or malicious behavior.</p>
<p>The fact that our data points are available as a stream means that we cannot &ldquo;look&rdquo; at them more than once. Therefore we need an algorithm with complexity linear to the number of points of the stream.</p>
<p>In this article, we&rsquo;ll build an algorithm for this task and apply it to detect anomalous behavior in the KDD Cup &lsquo;99 dataset which contains data from network connections to build a <em>network intrusion detector</em>.</p>
<p>We will model our data using a simple Gaussian-distribution model.</p>
<p>I thought it would be fun to implement it in one line of Bash. It turned out that squeezing everything in a huge line is not so fun, so I expanded it in a few more lines ðŸ™ƒ.</p>
<h3 id="gaussian-model-to-detect-anomalies">
  Gaussian model to detect anomalies
  <a class="heading-link" href="#gaussian-model-to-detect-anomalies">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>Without further ado, let&rsquo;s begin with some theory. We start by assuming that our data are following the Gaussian distribution.</p>
<p>Therefore, given an instance \( x \sim \mathcal{N}(\mu, \Sigma), x \in \mathbb{R}^d, x=[x_1, x_2, \ldots, x_d]^T \), its probability density function is:</p>
<p>\[  p(x; \mu, \Sigma)= \frac{1}{(2\pi)^{d/2}|\Sigma|^{\frac{1}{2}}}\exp\bigg(-\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\bigg) \]</p>
<p>where \(\mu \in \mathbb{R}^d\) is the mean and \( \Sigma \in \mathbb{R}^{d \times d} \) is the
covariance matrix.</p>
<p>Now assuming that variables \( x_i \sim \mathcal{N}(\mu_i, \sigma_i^{2}) \) are all independent, we get:</p>
<p>\[ p(x) = p(x_1, x_2, \ldots, x_d) = p(x_1; \mu_1, \sigma_1^2)p(x_2; \mu_2, \sigma_2^2)\ldots p(x_d; \mu_d,  \sigma_d^2) = \prod_{j=1}^{d} p(x_j; \mu_j, \sigma_j^2) \]
where:</p>
<p>\[ p(x; \mu, \sigma^2)=\frac{1}{\sqrt{2\pi}\sigma} \exp\Big(- \frac{(x-\mu)^2}{2\sigma^2}\Big) \]</p>
<p>To train the model (which basically consists of the values \( \mu_i, \sigma_i^2, \forall i \in [1, \ldots, d] \)), one needs to calculate the following
parameters (MLE):</p>
<ul>
<li>\( \mu_i = \frac{1}{n} \sum_{j=1}^{n}{x_i^{(j)}} \), where \( n \) is the number of training examples (the size of the training dataset),</li>
<li>\( \sigma_i^2 = \frac{1}{n}{\sum_{j=1}^{n}{(x_i^{(j)} - \mu_i)^2}} \)</li>
</ul>
<p>Then, in the evaluation phase, given a new example \( x \), we compute:</p>
<p>\[ p(x) = \prod_{i=1}^{d}{ \frac{1}{\sqrt{2\pi}\sigma_i} \exp\Big(- \frac{(x_i-\mu_i)^2}{2\sigma_i^2}\Big) } \]
and we flag \(x\) as anomaly if the value of \( p(x) \) is smaller than a threshold value \(\epsilon\) (hyperparameter).</p>
<p>Now here is the <strong>imporant part</strong>. How will we compute these values incrementally?</p>
<p>By having the tuples \( T_i = \big( \sum_{j=1}^{n}{x_i^{(j)}}, ~ \sum_{j=1}^{n}{{(x_i^{(j)})}^2}, ~ n \big) \), where \( n \) is the count of the instances,
available at any point of the computation!
This way, when training or evaluating, in batch or on stream, we have access to all parameters of the model \( \mu_i \) and
\( \sigma_i^2 \) at any time by calculating:</p>
<p>\[ \mu_i = \frac{\sum_{j=1}^{n}{x_i^{(j)}}}{n} = \frac{T_i[0]}{T_i[2]}, ~~~~~
\sigma_i^2 = \frac{\sum_{j=1}^{n}{{(x_i^{(j)})}^2}}{n} - \mu_i^2 = \frac{T_i[1]}{T_i[2]} - (\frac{T_i[0]}{T_i[2]})^2 \]</p>
<p>So let&rsquo;s go ahead and fetch the dataset, then implement the algorithm with awk.</p>
<h3 id="fetch-and-prepare-the-dataset">
  Fetch and prepare the dataset
  <a class="heading-link" href="#fetch-and-prepare-the-dataset">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>The dataset is available <a href="http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html">here</a>. I will be using the 10 percent version, and only the HTTP protocol.</p>
<p>We&rsquo;ll use curl for the GET request, pipe it into zcat (because it is gzipped), and then use awk to keep only the HTTP rows, and filter out most features,
keeping only the 4 most important columns: <code>duration</code>, <code>src_bytes</code> and <code>dst_bytes</code>, plus the <code>label</code>. We also edit the label to be <code>1</code> in case of
anomaly, or <code>0</code>, in case of normal observation.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>URL=http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data_10_percent.gz <span style="font-style:italic"># 10 percent</span>
</span></span><span style="display:flex;"><span>OUTFILE=http.csv
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>curl $URL | zcat | awk <span style="font-style:italic">&#39;BEGIN{ FS=&#34;,&#34;; OFS=&#34;,&#34; } { LABEL=($NF==&#34;normal.&#34; ? 0 : 1); if($3 == &#34;http&#34;) print $1, $5, $6, LABEL }&#39;</span> &gt; $OUTFILE
</span></span></code></pre></div><p>So now our dataset with 64293 rows is in the http.csv file.</p>
<h3 id="implementation">
  Implementation
  <a class="heading-link" href="#implementation">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>We&rsquo;ll put our code in a file called <code>gauss.awk</code>. Start by defining some variables:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>BEGIN {
</span></span><span style="display:flex;"><span>    FS = <span style="font-style:italic">&#34;,&#34;</span>;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="font-style:italic"># &lt; NF to ignore the last column (label)</span>
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">for</span> (i = 1; i &lt; NF; i++) {
</span></span><span style="display:flex;"><span>        SUMS[i] = 0;
</span></span><span style="display:flex;"><span>        SQUARES[i] = 0;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p><code>FS</code> is a special variable used by awk, denoting the input field separator. Since we have a csv, we use commas.
<code>SUMS</code> and <code>SQUARES</code> are two arrays, with the one holding the cumulative sum of the values of each feature, and the other
holding the cumulative sum of the squares of these values.</p>
<p>Next we move on to the main awk loop, and write the code which will apply to every row of the input file.</p>
<p>Here we will first update the model, by updating the <code>SUMS</code> and <code>SQUARES</code> arrays. For each feature (or column, call it
however you like), we update the respective value using the current row that awk is parsing.</p>
<p>After that, we use the current row and the model (the arrays), to calculate an anomaly score, according to the formula
we discussed earlier (see \( p(x) \)). To get an accurate value of \( \pi \) for the calculations, I am using <code>atan2(0, -1)</code>.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>{
</span></span><span style="display:flex;"><span>    <span style="font-style:italic"># Update the model</span>
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">for</span> (i = 1; i &lt; NF; i++) {
</span></span><span style="display:flex;"><span>        SUMS[i] += $i;
</span></span><span style="display:flex;"><span>        SQUARES[i] += $i ** 2;
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="font-style:italic"># Calculate score using the model</span>
</span></span><span style="display:flex;"><span>    score = 1;
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">for</span> (i = 1; i &lt; NF; i++) {
</span></span><span style="display:flex;"><span>        mu = SUMS[i] / NR;
</span></span><span style="display:flex;"><span>        sigma2 = (SQUARES[i] / NR) - mu ** 2;
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">if</span> (sigma2 == 0)
</span></span><span style="display:flex;"><span>            score = 0;
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">else</span> {
</span></span><span style="display:flex;"><span>            score *= exp(- (($i - mu) ** 2) / (2 * sigma2)) / sqrt(2 * atan2(0, -1) * sigma2);
</span></span><span style="display:flex;"><span>        }
</span></span><span style="display:flex;"><span>    }
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="font-style:italic"># Output the score</span>
</span></span><span style="display:flex;"><span>    print score, $NF
</span></span><span style="display:flex;"><span>}
</span></span></code></pre></div><p>Finally, we output the score, along with the actual label.</p>
<p>Now let&rsquo;s run this:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ awk -f gauss.awk http.csv
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>2.01308e-12 0
</span></span><span style="display:flex;"><span>2.02266e-12 0
</span></span><span style="display:flex;"><span>2.04232e-12 0
</span></span><span style="display:flex;"><span>2.04342e-12 0
</span></span><span style="display:flex;"><span>2.0474e-12 0
</span></span><span style="display:flex;"><span>2.02895e-12 0
</span></span><span style="display:flex;"><span>2.0324e-12 0
</span></span><span style="display:flex;"><span>2.03025e-12 0
</span></span></code></pre></div><p>Looks good. Some warnings on stderr about the <code>exp()</code> function are normal, it is because some arguments passed in <code>exp</code> are <code>&lt; -745</code> and
therefore return incredibly small values but awk handles them as <code>0</code> so we&rsquo;re good.</p>
<p>Now I want to calculate the ROC AUC score, and for that, I will be using a simple python script with the <code>numpy</code> module,
and the <code>roc_auc_score</code> function from the <code>scikit-learn</code> module:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">import</span> <span style="font-weight:bold">numpy</span> <span style="font-weight:bold">as</span> <span style="font-weight:bold">np</span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="font-weight:bold">sklearn.metrics</span> <span style="font-weight:bold">import</span> roc_auc_score
</span></span><span style="display:flex;"><span><span style="font-weight:bold">from</span> <span style="font-weight:bold">sys</span> <span style="font-weight:bold">import</span> stdin
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>preds = []
</span></span><span style="display:flex;"><span>actuals = []
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="font-weight:bold">for</span> line <span style="font-weight:bold">in</span> stdin:
</span></span><span style="display:flex;"><span>    s = line.split(<span style="font-style:italic">&#39; &#39;</span>)
</span></span><span style="display:flex;"><span>    preds.append(-float(s[0]))
</span></span><span style="display:flex;"><span>    actuals.append(int(s[1]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>preds_arr = np.array(preds)
</span></span><span style="display:flex;"><span>actuals_arr = np.array(actuals)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(roc_auc_score(actuals_arr, preds_arr))
</span></span></code></pre></div><p>Now I can pipe the output of awk into the script, which I&rsquo;ll call <code>auroc.py</code>, like this:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ awk -f gauss.awk  http.csv | python3 auroc.py
</span></span></code></pre></div><p>, and this prints the score, which is <code>0.9762313543238387</code>, a pretty good value.</p>
<p>Last but not least, we can even choose a threshold and start flagging the values. This is what an anomaly detector actually does. What&rsquo;s the
intuition behind this? Since the scores are nothing more than the probability of the observation in the Gaussian distribution (the model),
we will be flagging the value as anomaly, if this score is lower than a particular threshold, meaning that it is having a low probability
of occurring.</p>
<p>A good threshold here could be <code>9.83056225277626e-15</code> (found with some interpolation), so we can just change the last lines of the script to:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span><span style="font-weight:bold">if</span> (score &lt; 9.83056225277626e-15)
</span></span><span style="display:flex;"><span>    print 1, $NF
</span></span><span style="display:flex;"><span><span style="font-weight:bold">else</span>
</span></span><span style="display:flex;"><span>    print 0, $NF
</span></span></code></pre></div><p>, so that it prints <code>1</code> if the score is lower than the threshold, or <code>0</code> if is not, along with the actual label. You can now calculate the F1 score if
you want with some <code>grep</code>s of the output.</p>
<h3 id="putting-it-all-together">
  Putting it all together
  <a class="heading-link" href="#putting-it-all-together">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>Now let&rsquo;s glue everything together in one line:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>$ curl -s $URL | zcat | awk <span style="font-style:italic">&#39;BEGIN{ FS=&#34;,&#34;; OFS=&#34;,&#34; } { LABEL=($NF==&#34;normal.&#34; ? 0 : 1); if($3 == &#34;http&#34;) print $1, $5, $6, LABEL }&#39;</span> | awk -f gauss.awk 2&gt; /dev/null | python3 auroc.py
</span></span><span style="display:flex;"><span>0.9762313543238387
</span></span></code></pre></div><p>I got rid of the <code>http.csv</code> file and piped the output of the first awk directly into the second one, and I also redirected the stderr of the second awk to <code>/dev/null</code> to silence the warnings, so that we can get a clean output in the terminal.</p>
<p>So there you go, one pass on the data with a few lines of awk are enough to handle many cases of spotting outliers in datasets.</p>
<h3 id="sidenote-big-data-frameworks">
  Sidenote: Big Data Frameworks
  <a class="heading-link" href="#sidenote-big-data-frameworks">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>This algorithm can be implemented in any Big Data framework (Apache Spark, Hadoop, Flink, whatever&hellip;) in a very straightforward way, with the functional
operators <code>map</code> and <code>reduce</code>. Assume that our training data are \( n \) d-dimensional
vectors \( x^{(j)} = [x_1^{(j)}, x_2^{(j)}, \ldots, x_d^{(j)}]^T, j \in [1, \ldots, n] \).
Then the values of vectors \( \mu \) and \( \sigma^2 \) can be computed by:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-scala" data-lang="scala"><span style="display:flex;"><span><span style="font-style:italic">// Scala
</span></span></span><span style="display:flex;"><span><span style="font-style:italic"></span>dataSet
</span></span><span style="display:flex;"><span>  .map { x <span style="font-weight:bold">=&gt;</span> (x, pow(x, 2), 1) }
</span></span><span style="display:flex;"><span>  .reduce { (x1, x2) <span style="font-weight:bold">=&gt;</span> (x1._1 + x2._1, x1._2 + x2._2, x1._3 + x2._3) }
</span></span><span style="display:flex;"><span>  .collect()
</span></span></code></pre></div>
      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        Â©
        
          2018 -
        
        2024
        
      
      
      
    </section>
  </footer>


    </main>

    
      
      <script src="/js/coder.min.cb0c595e02234420f3ad3886bf4a9bd2874d0e1e78e090138a9ef158b35aaf17.js" integrity="sha256-ywxZXgIjRCDzrTiGv0qb0odNDh544JATip7xWLNarxc="></script>
    

    
      <script src="/js/custom.js"></script>
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
      <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.16.2/build/highlight.min.js"></script>
    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-122370931-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    

    

    

    

    

    
  </body>

</html>
