<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">
    <meta name="color-scheme" content="light dark">

    
      <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests; block-all-mixed-content; default-src 'self'; child-src 'self'; font-src 'self' https://fonts.gstatic.com https://cdn.jsdelivr.net/ https://cdnjs.cloudflare.com; form-action 'self'; frame-src 'self'; img-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com/ https://cdn.jsdelivr.net/; script-src 'self' 'unsafe-inline' https://www.google-analytics.com https://cdn.jsdelivr.net https://cdnjs.cloudflare.com; prefetch-src 'self'; connect-src 'self' https://www.google-analytics.com;">

    

    
    <meta name="description" content="Introduction Volume rendering is a collection of methods to visualize volumes, which we often encounter in scientific, engineering, and biomedical domains.
Volume datasets are essentially sampled 3D scalar fields: \( f[m, n, k]: \mathbb{N^3} \rightarrow \mathbb{N} \). We refer to the pairs of 3D coordinates and their corresponding intensity values as Voxels (the 3D equivalent of pixels).
Volume rendering can be divided in three main categories:
2D visualization of slices of the volume, known as MPR (Multi-Planar Reconstruction) Indirect 3D visualization - Isosurfaces Direct 3D visualization known as DVR (Direct Volume Rendering) Let&rsquo;s examine each category separately.">
    <meta name="keywords" content="">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Volume Rendering for 3D Data Visualization"/>
<meta name="twitter:description" content="Introduction Volume rendering is a collection of methods to visualize volumes, which we often encounter in scientific, engineering, and biomedical domains.
Volume datasets are essentially sampled 3D scalar fields: \( f[m, n, k]: \mathbb{N^3} \rightarrow \mathbb{N} \). We refer to the pairs of 3D coordinates and their corresponding intensity values as Voxels (the 3D equivalent of pixels).
Volume rendering can be divided in three main categories:
2D visualization of slices of the volume, known as MPR (Multi-Planar Reconstruction) Indirect 3D visualization - Isosurfaces Direct 3D visualization known as DVR (Direct Volume Rendering) Let&rsquo;s examine each category separately."/>

    <meta property="og:title" content="Volume Rendering for 3D Data Visualization" />
<meta property="og:description" content="Introduction Volume rendering is a collection of methods to visualize volumes, which we often encounter in scientific, engineering, and biomedical domains.
Volume datasets are essentially sampled 3D scalar fields: \( f[m, n, k]: \mathbb{N^3} \rightarrow \mathbb{N} \). We refer to the pairs of 3D coordinates and their corresponding intensity values as Voxels (the 3D equivalent of pixels).
Volume rendering can be divided in three main categories:
2D visualization of slices of the volume, known as MPR (Multi-Planar Reconstruction) Indirect 3D visualization - Isosurfaces Direct 3D visualization known as DVR (Direct Volume Rendering) Let&rsquo;s examine each category separately." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nikosg.com/volume-rendering-for-3d-data-visualization/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-03-12T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-03-12T00:00:00+00:00" />


    <title>
  Volume Rendering for 3D Data Visualization · Nikos Gavalas
</title>

    
      <link rel="canonical" href="https://nikosg.com/volume-rendering-for-3d-data-visualization/">
    

    <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>

    
      
      
      <link rel="stylesheet" href="/css/coder.min.708686f8ab8176e91d44fcbe488a0fe0333b94f405cf18a52383d67ba22f0ccb.css" integrity="sha256-cIaG&#43;KuBdukdRPy&#43;SIoP4DM7lPQFzxilI4PWe6IvDMs=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="/css/coder-dark.min.aa883b9ce35a8ff4a2a5008619005175e842bb18a8a9f9cc2bbcf44dab2d91fa.css" integrity="sha256-qog7nONaj/SipQCGGQBRdehCuxioqfnMK7z0Tastkfo=" crossorigin="anonymous" media="screen" />
      
    

    
      <link rel="stylesheet" href="/css/custom.css" />
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.16.2/build/styles/default.min.css" />
    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    

    <meta name="generator" content="Hugo 0.111.3">
  </head>

  
  
    
  
  <body class="preload-transitions colorscheme-auto"
        onload=""
  >
    
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Nikos Gavalas
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Posts</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://nikosg.com/volume-rendering-for-3d-data-visualization/">
              Volume Rendering for 3D Data Visualization
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2022-03-12T00:00:00Z'>
                March 12, 2022
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              6-minute read
            </span>
          </div>
          
          <div class="categories">
  <i class="fa fa-folder" aria-hidden="true"></i>
    <a href="/categories/algorithms/">algorithms</a>
      <span class="separator">•</span>
    <a href="/categories/data/">data</a>
      <span class="separator">•</span>
    <a href="/categories/visualization/">visualization</a></div>

          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/algorithms/">algorithms</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/programming/">programming</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/data/">data</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/visualization/">visualization</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <h2 id="introduction">
  Introduction
  <a class="heading-link" href="#introduction">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>Volume rendering is a collection of methods to visualize volumes, which we often encounter in scientific, engineering, and biomedical domains.</p>
<p>Volume datasets are essentially sampled 3D scalar fields: \( f[m, n, k]:  \mathbb{N^3} \rightarrow \mathbb{N} \). We refer to the pairs of 3D coordinates and their corresponding intensity values as <em>Voxels</em> (the 3D equivalent of pixels).</p>
<p>Volume rendering can be divided in three main categories:</p>
<ul>
<li>2D visualization of slices of the volume, known as MPR (Multi-Planar Reconstruction)</li>
<li>Indirect 3D visualization - Isosurfaces</li>
<li>Direct 3D visualization known as DVR (Direct Volume Rendering)</li>
</ul>
<p>Let&rsquo;s examine each category separately. All the examples presented below are generated on the 3D carp CT dataset, a popular volumetric dataset in the literature, often used as a benchmark in 3D volume rendering.</p>
<h2 id="multi-planar-reconstructionreformatting-mpr">
  Multi-Planar Reconstruction/Reformatting (MPR)
  <a class="heading-link" href="#multi-planar-reconstructionreformatting-mpr">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>This is a relatively simple method, where we generate a view alongside a plane perpendicular to the camera, through the center of the volume.</p>
<p>An example of this method can be seen below:</p>
<img class="img-center" src="/images/volvis/1a.png" alt="MPR Example" style="width:50%;"/>
<p>This image is the rendering of a &ldquo;slice&rdquo; of a carp (using Maximum Intensity Projection - we explain this in the DVR section). MPR is quite limiting because the view-plane has to pass through the center of the volume.</p>
<h2 id="direct-volume-rendering-dvr">
  Direct Volume Rendering (DVR)
  <a class="heading-link" href="#direct-volume-rendering-dvr">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>There exist two main methods to directly render voxels: <strong>ray-casting</strong> and <strong>voxel-projection</strong>. Ray-casting is most commonly used, and GPUs are great at it. It works as follows:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-fallback" data-lang="fallback"><span style="display:flex;"><span>for each pixel of the screen:
</span></span><span style="display:flex;"><span>    shoot a ray through the pixel and gather equidistant samples of the volume
</span></span><span style="display:flex;"><span>    for each sample:
</span></span><span style="display:flex;"><span>        calculate the position
</span></span><span style="display:flex;"><span>        interpolate the intensity at the position
</span></span><span style="display:flex;"><span>    calculate color along the ray by aggregating the intensities sampled
</span></span></code></pre></div><p>There are two ways to aggregate the intensities along a ray:</p>
<ul>
<li>Maximum Intensity Projection (MIP) - takes the maximum value out of all the samples</li>
<li>Average Intensity Projection (X-ray like) - integrates all the values along the ray</li>
</ul>
<h3 id="interpolation">
  Interpolation
  <a class="heading-link" href="#interpolation">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>When sampling along a ray, the samples almost always will not correspond directly to voxels of the volume. Hence, we need a way to estimate a value for the sample for the neighboring voxels.</p>
<p>The simplest way would be to just assign the value of the neighboring voxel. In fact, the first figure we saw above, in the MPR section, was generated this way. This is known as Nearest-Neighbor (NN) interpolation.</p>
<p>A more sophisticated approach would be to assume that the inter-sample values change linearly. This approach is known as Linear Interpolation and, since we operate with spatial values, Tri-Linear Interpolation - because we need to linearly interpolate along 3 dimensions.</p>
<p>Below is the same object as the one shown in the previous section, except it was rendered using Tri-Linear Interpolation to demonstrate the difference:</p>
<img class="img-center" src="/images/volvis/1b.png" alt="Tri-Linear Interpolation" style="width:50%;"/>
<p>As we can see the edges are a lot smoother and the image overall is less &ldquo;pixelated&rdquo;.</p>
<h3 id="compositing---emission-absorption-model">
  Compositing - Emission-Absorption Model
  <a class="heading-link" href="#compositing---emission-absorption-model">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>With all the intensity projections we talked so far, we risk saturating the rendered image we get from the volume. This is why we use Compositing, an idea which simulates physical light transport through a semi-transparent medium that emits/absorbs/scatters light.</p>
<p>In this model, the accumulated color \(C&rsquo;\) at position \(i\) is calculated recursively/iteratively as follows (back-to-front), using the opacity \(A_i\) at position \(i\):</p>
<p>\[ C^\prime_{i} = C_i + (1 - A_{i})C^\prime_{i+1} \]</p>
<h4 id="associated-colors">
  Associated colors
  <a class="heading-link" href="#associated-colors">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h4>
<p>In the formula above, the colors are not plain RGBA vectors but rather <em>opacity-weighted</em> RGBA vectors. These are called &ldquo;Associated Colors&rdquo;:</p>
<p>\[c_i = \begin{bmatrix}R_i\\G_i\\B_i\\A_i\end{bmatrix}, C_i = \begin{bmatrix} R_iA_i\\G_iA_i\\B_iA_i\\A_i \end{bmatrix}\]</p>
<h4 id="transfer-functions">
  Transfer functions
  <a class="heading-link" href="#transfer-functions">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h4>
<p>What if we want to visualize only specific intensity values of the volume? This would allow us to separate the volume into segments and make its various parts stand out, while also hiding other parts by setting their opacity to zero and making them transparent. For example, if our volume data are CT scans, we could specify the color of the bones to be different from the color of tissue.</p>
<p>We can achieve this using <strong>Transfer Functions</strong> (TF). A TF maps the (interpolated) voxel intensity values from the volume into user-defined RGB/opacity values, <em>before compositing</em>. Let&rsquo;s demonstrate this in the carp dataset. Using a monochromatic TF, we can get the following result:</p>
<img class="img-center" src="/images/volvis/4a.png" alt="Monochromatic 1D TF" style="width:50%;"/>
<p>If we map to RGB with a different TF that uses white for lower intensity values and yellow for higher ones:</p>
<img class="img-center" src="/images/volvis/4b.png" alt="RGB 1D TF" style="width:50%;"/>
<p>Notice how the bones are visually separable from the tissue and the air bladder is also clearly visible.</p>
<p>The TF can also be multi-dimensional, leveraging additional volume properties. Such a property could be the magnitude of the volume gradient, which may be used to capture &ldquo;boundarieness&rdquo;. This can help us better visualize surfaces, as demonstrated below:</p>
<img class="img-center" src="/images/volvis/5a.png" alt="2D TF" style="width:50%;"/>
<p>Observe how clearly the internal surfaces are outlined in comparison to the previous methods.</p>
<h2 id="indirect-3d-visualization---isosurfaces">
  Indirect 3D visualization - Isosurfaces
  <a class="heading-link" href="#indirect-3d-visualization---isosurfaces">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>Volume visualization using Isosurface Raycasting is simple: we sample the volume using rays like before i.e. we iterate along a line through the volume, except we stop when we encounter a value that is larger than a certain isovalue and we assign this value to our view pixel.</p>
<p>The result is conceptually and visually similar to a shadow cast by the volume onto our view:</p>
<img class="img-center" src="/images/volvis/2a.png" alt="Isosurface with low isovalue" style="width:50%;"/>
<p>Setting a higher isovalue, we get an <em>isosurface</em> that outlines parts of the volume with higher intensities:</p>
<img class="img-center" src="/images/volvis/2b.png" alt="Isosurface with high isovalue" style="width:50%;"/>
<h3 id="bisection-algorithm">
  Bisection Algorithm
  <a class="heading-link" href="#bisection-algorithm">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>To better estimate the exact position of the isosurface, we can use binary search on the ray using the volumes&rsquo; intensity values. This is known as <em>Bisection Algorithm</em> and yields more accurate and more detailed surfaces.</p>
<h3 id="phong-shading">
  Phong shading
  <a class="heading-link" href="#phong-shading">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>To actually visualize the surfaces as 3D, we need a shading model. The shading model will change the colors of the voxels of the isosurface to darker/lighter tones according to the volume gradients, camera position and light position to delineate the texture of the surface.</p>
<p>One of the most popular shading models is the <a href="https://en.wikipedia.org/wiki/Phong_reflection_model">Phong shading model</a>. According to it, the final color of a point of a surface is equal to the sum of three different components: <em>Ambient</em>, <em>Diffuse</em> and <em>Specular</em>:</p>
<img class="img-center" src="/images/volvis/phong-a.png" alt="Phong model" />
<p>Formally, for a single light source:</p>
<p>\[ I_p = k_ai + k_d (\hat{L}\cdot \hat{n})i + k_s(\hat{R} \cdot \hat{V})^\alpha i \]</p>
<p>, where \( k_a, k_d, k_s, \alpha \) constants (describing the material), \(i\) the color of the voxel, \(\hat{L}\) the light vector and \(\hat{R}\) its reflection given by \( \hat{R} = 2(\hat{L}\cdot \hat{n})\hat{n} - \hat{L} \), \(\hat{n}\) (or \(\hat{G}\)) the normal vector of the surface on the point, and \( \hat{V} \) the camera position. Hats (\(\hat{}\)) denote normalized vectors.</p>
<p>The vectors are pictured in the figure below:</p>
<img class="img-center" src="/images/volvis/phong-b.png" alt="Phong model 2" style="width:50%;"/>
<p>When used with compositing, shading is applied <em>before compositing</em> and <em>after applying the TF</em>.</p>
<p>By applying Phong shading and the Bisection Algorithm, we get the following results on our dataset&rsquo;s isosurfaces, for the isovalues we used earlier:</p>
<!-- Align images side-by-side -->
<img src="/images/volvis/3a.png" alt="Phong shading 1" style="display:inline-block;float:left;width:50%;padding:2%;"/>
<img src="/images/volvis/3b.png" alt="Phong shading 2" style="display:inline-block;width:50%;padding:2%;"/>
<h3 id="gradients">
  Gradients
  <a class="heading-link" href="#gradients">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>Until now we mentioned the volume gradients multiple times. How do we calulate them?
The gradients can be calculated from the original data with central differences:</p>
<p>\[ G_x[i,j,k] = \frac{f[i+1,j,k] - f[i-1,j,k]}{2\Delta x} \]
\[ G_y[i,j,k] = \frac{f[i,j+1,k] - f[i,j-1,k]}{2\Delta y} \]
\[ G_z[i,j,k] = \frac{f[i,j,k+1] - f[i,j,k-1]}{2\Delta z} \]</p>
<p>\[ \vec{G} = \begin{bmatrix} G_x \\ G_y \\ G_z\end{bmatrix} \approx \vec{n} \]</p>
<p>They can then be cached/stored if enough memory/storage is available, in order to avoid recalculation every time which is computationally intensive.</p>
<h2 id="notes">
  Notes
  <a class="heading-link" href="#notes">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>I wrote this blogpost with the stuff I learned from the MSc course &ldquo;Data Visualization&rdquo; offered at TU Delft. It was a very interesting and well-organized course. The code for the Volume Visualization project (containing my implementations for the algorithms I used in this post) can be found <a href="https://github.com/nikosgavalas/tud-datavis/blob/master/volvis/VolVis_Project_Framework/src/volume/volume.cpp">here</a> and <a href="https://github.com/nikosgavalas/tud-datavis/blob/master/volvis/VolVis_Project_Framework/src/render/renderer.cpp">here</a>.</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2018 -
        
        2024
        
      
      
      
    </section>
  </footer>


    </main>

    
      
      <script src="/js/coder.min.cb0c595e02234420f3ad3886bf4a9bd2874d0e1e78e090138a9ef158b35aaf17.js" integrity="sha256-ywxZXgIjRCDzrTiGv0qb0odNDh544JATip7xWLNarxc="></script>
    

    
      <script src="/js/custom.js"></script>
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
      <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.16.2/build/highlight.min.js"></script>
    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-122370931-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    

    

    

    

    

    
  </body>

</html>
