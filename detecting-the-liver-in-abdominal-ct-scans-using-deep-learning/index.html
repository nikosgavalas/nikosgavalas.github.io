<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta http-equiv="Content-Language" content="en">
    <meta name="color-scheme" content="light dark">

    
      <meta http-equiv="Content-Security-Policy" content="upgrade-insecure-requests; block-all-mixed-content; default-src 'self'; child-src 'self'; font-src 'self' https://fonts.gstatic.com https://cdn.jsdelivr.net/ https://cdnjs.cloudflare.com; form-action 'self'; frame-src 'self'; img-src 'self'; object-src 'none'; style-src 'self' 'unsafe-inline' https://fonts.googleapis.com/ https://cdn.jsdelivr.net/; script-src 'self' 'unsafe-inline' https://www.google-analytics.com https://cdn.jsdelivr.net https://cdnjs.cloudflare.com; prefetch-src 'self'; connect-src 'self' https://www.google-analytics.com;">

    

    
    <meta name="description" content="This blogpost is part of the group project of the course &ldquo;Computer Vision by Deep Learning&rdquo;, offered at TU Delft. Credits to my collaborator Markos Gkozntaris. The codebase of our project can be found here.
Introduction In Computer Vision, there exists no network architecture that is equally suitable and effective for all tasks. Different architectures can be used in order to tackle tasks in specific applications and datasets.
In this blogpost, we discuss the effectiveness of UNet3&#43; on datasets of CT scans, on the task of performing liver segmentation.">
    <meta name="keywords" content="">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Detecting the liver in abdominal CT scans using Deep Learning"/>
<meta name="twitter:description" content="This blogpost is part of the group project of the course &ldquo;Computer Vision by Deep Learning&rdquo;, offered at TU Delft. Credits to my collaborator Markos Gkozntaris. The codebase of our project can be found here.
Introduction In Computer Vision, there exists no network architecture that is equally suitable and effective for all tasks. Different architectures can be used in order to tackle tasks in specific applications and datasets.
In this blogpost, we discuss the effectiveness of UNet3&#43; on datasets of CT scans, on the task of performing liver segmentation."/>

    <meta property="og:title" content="Detecting the liver in abdominal CT scans using Deep Learning" />
<meta property="og:description" content="This blogpost is part of the group project of the course &ldquo;Computer Vision by Deep Learning&rdquo;, offered at TU Delft. Credits to my collaborator Markos Gkozntaris. The codebase of our project can be found here.
Introduction In Computer Vision, there exists no network architecture that is equally suitable and effective for all tasks. Different architectures can be used in order to tackle tasks in specific applications and datasets.
In this blogpost, we discuss the effectiveness of UNet3&#43; on datasets of CT scans, on the task of performing liver segmentation." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://nikosg.com/detecting-the-liver-in-abdominal-ct-scans-using-deep-learning/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-07-13T00:00:00+00:00" />
<meta property="article:modified_time" content="2022-07-13T00:00:00+00:00" />


    <title>
  Detecting the liver in abdominal CT scans using Deep Learning · Nikos Gavalas
</title>

    
      <link rel="canonical" href="https://nikosg.com/detecting-the-liver-in-abdominal-ct-scans-using-deep-learning/">
    

    <link rel="preload" href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as="font" type="font/woff2" crossorigin>

    
      
      
      <link rel="stylesheet" href="/css/coder.min.708686f8ab8176e91d44fcbe488a0fe0333b94f405cf18a52383d67ba22f0ccb.css" integrity="sha256-cIaG&#43;KuBdukdRPy&#43;SIoP4DM7lPQFzxilI4PWe6IvDMs=" crossorigin="anonymous" media="screen" />
    

    

    
      
        
        
        <link rel="stylesheet" href="/css/coder-dark.min.aa883b9ce35a8ff4a2a5008619005175e842bb18a8a9f9cc2bbcf44dab2d91fa.css" integrity="sha256-qog7nONaj/SipQCGGQBRdehCuxioqfnMK7z0Tastkfo=" crossorigin="anonymous" media="screen" />
      
    

    
      <link rel="stylesheet" href="/css/custom.css" />
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.16.2/build/styles/default.min.css" />
    

    

    <link rel="icon" type="image/png" href="/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="/images/favicon-16x16.png" sizes="16x16">

    <link rel="apple-touch-icon" href="/images/apple-touch-icon.png">
    <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">

    

    <meta name="generator" content="Hugo 0.111.3">
  </head>

  
  
    
  
  <body class="preload-transitions colorscheme-auto"
        onload=""
  >
    
<div class="float-container">
    <a id="dark-mode-toggle" class="colorscheme-toggle">
        <i class="fa fa-adjust fa-fw" aria-hidden="true"></i>
    </a>
</div>


    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="/">
      Nikos Gavalas
    </a>
    
      <input type="checkbox" id="menu-toggle" />
      <label class="menu-button float-right" for="menu-toggle">
        <i class="fa fa-bars fa-fw" aria-hidden="true"></i>
      </label>
      <ul class="navigation-list">
        
          
            <li class="navigation-item">
              <a class="navigation-link" href="/posts/">Posts</a>
            </li>
          
        
        
      </ul>
    
  </section>
</nav>


      <div class="content">
        
  <section class="container post">
    <article>
      <header>
        <div class="post-title">
          <h1 class="title">
            <a class="title-link" href="https://nikosg.com/detecting-the-liver-in-abdominal-ct-scans-using-deep-learning/">
              Detecting the liver in abdominal CT scans using Deep Learning
            </a>
          </h1>
        </div>
        <div class="post-meta">
          <div class="date">
            <span class="posted-on">
              <i class="fa fa-calendar" aria-hidden="true"></i>
              <time datetime='2022-07-13T00:00:00Z'>
                July 13, 2022
              </time>
            </span>
            <span class="reading-time">
              <i class="fa fa-clock-o" aria-hidden="true"></i>
              11-minute read
            </span>
          </div>
          
          <div class="categories">
  <i class="fa fa-folder" aria-hidden="true"></i>
    <a href="/categories/deep-learning/">deep learning</a></div>

          <div class="tags">
  <i class="fa fa-tag" aria-hidden="true"></i>
    <span class="tag">
      <a href="/tags/ai/">ai</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/computer-vision/">computer vision</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/deep-learning/">deep learning</a>
    </span>
      <span class="separator">•</span>
    <span class="tag">
      <a href="/tags/visualization/">visualization</a>
    </span></div>

        </div>
      </header>

      <div>
        
        <p>This blogpost is part of the group project of the course &ldquo;Computer Vision by Deep Learning&rdquo;, offered at TU Delft. Credits to my collaborator <a href="https://github.com/markos-gkozntaris">Markos Gkozntaris</a>. The codebase of our project can be found <a href="https://github.com/markos-gkozntaris/Does-UNet3-Generalize">here</a>.</p>
<h2 id="introduction">
  Introduction
  <a class="heading-link" href="#introduction">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>In Computer Vision, there exists no network architecture that is equally suitable and effective for all tasks. Different architectures can be used in order to tackle tasks in specific applications and datasets.</p>
<p>In this blogpost, we discuss the effectiveness of UNet3+ on datasets of CT scans, on the task of performing liver segmentation.</p>
<p>A direct comparison of <a href="https://https://arxiv.org/abs/1505.04597">UNet</a> [1], and an ablation study of its extension <a href="https://https://arxiv.org/abs/2004.08790">UNet3+</a> [2], will be addressed. Unet3+ was specifically designed for organ semantic segmentations in CT scans, i.e. associating each pixel of a CT slice with its class label. More specifically, the liver organ will be classsifed, using the same loss function in both cases.</p>
<p>The repository containing our implemetations of the models, as well as the data processing, evaluation and training scripts can be found <a href="https://github.com/markos-gkozntaris/Does-UNet3-Generalize">here</a>.</p>
<h2 id="dataset">
  Dataset
  <a class="heading-link" href="#dataset">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>The dataset we used is comprised of 130 CT scans of the torso area, from the LiTS17 challenge (Liver Tumor Segmentation Challenge) [3]. The dataset also provides the segmentation masks for the liver. Its total size is around 50GB, including the masks.</p>
<p>Each of these volumes is a DICOM file containing an arbitrary number of 512x512 images, with this number ranging from 75 to 624, and each of these images corresponds to a different vertical &ldquo;slice&rdquo; of the torso. The values of the pixels belong to the <a href="https://en.wikipedia.org/wiki/Hounsfield_scale">Hounsfield scale</a>.</p>
<p>For example, let&rsquo;s visualize a slice (n. 55) from the first volume of the dataset:</p>
<img class="img-center" src="/assets/dataset-vis-1.png" alt="Dataset Visualization 1" />
<p>Now because we want to specifically target the liver, we use the values of the Hounsfield scale that correspond to it as a filter. The result is the following:</p>
<img class="img-center" src="/assets/dataset-vis-2.png" alt="Dataset Visualization 2" />
<p>Finally, we downsampled the slices and masks to 256 by max pooling, to decrease the size of the input. As can be seen in the resulting image, the information loss due to downsampling is tolerable:</p>
<img class="img-center" src="/assets/dataset-vis-3.png" alt="Dataset Visualization 3" />
<p>To use this dataset more efficiently with PyTorch, we created a class extending PyTorch&rsquo;s DataSet class and created indices for it that allow random access.</p>
<h2 id="instance-segmentation">
  Instance segmentation
  <a class="heading-link" href="#instance-segmentation">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>Before diving in the model architectures it would be wise to define the computer vision task that they aim to solve, the task of semantic segmentation.</p>
<p>Semantic segmentation, or image segmentation, is the task of clustering parts of an image together which belong to the same object class. It is a form of pixel-level prediction because each pixel in an image is classified according to a category, making the labels class-aware [4]. Visualy, this can be seen in the image below (<a href="https://ai-pool.com/d/could-you-explain-me-how-instance-segmentation-works">source</a>), where also other computer vision taks are presented for comparison.</p>
<img class="img-center" src="/assets/image-seg.png" alt="Image Segmentation" />
<h2 id="models">
  Models
  <a class="heading-link" href="#models">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>The two main architectures compared in this blog will be described, starting from UNet and continuing with its extension UNet3+ (which builds on top of the original UNet).</p>
<h3 id="unet">
  UNet
  <a class="heading-link" href="#unet">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>UNet is a U-shaped encoder-decoder network architecture, which consists of four encoder blocks and four decoder blocks.</p>
<p>Its first path, the encoder, is used to capture the context in the image (serving as a feature extractor) and comprises of convolutional and max pooling layers. More specifically each encoder block consists of two 3x3 convolutions, each followed by a ReLU activation function in order to introduce non-linearity in the network, which helps in the better generalization of the training data. The output of this stage is fed to the same stage of the decoder part of the network, and is refered to as a skip connection.</p>
<p>This double convolution is followed by a 2x2 max-pooling, where the spatial dimensions (height and width) of the feature maps are reduced by half.</p>
<p>The 5-th stage of the network is a connector between the encoder and the decoder, consisiting of only two 3x3 convolutions, each followed by a ReLU activation function.</p>
<p>On the other hand, the decoder, which is a symmetric expanding of the first path, comprises of upsamling and transpose convolution layers. It is used to enable precised localization. On a high level, its main goal is to take an abstract representation of the encoder and generate a semantic segmentation mask.
The decoder block starts with a 2x2 transpose convolution, which is then conatenated with the corresponding skip connection feature map from the encoder block. Next, these concatenations are followed by two 3x3 convolutional layers, with each followed by a ReLU activation function.</p>
<p>The output of the last decoder passes through a 1x1 convolution with sigmoid function.</p>
<img class="img-center" src="/assets/unet.png" alt="UNet" />
<p>It is worth noting that the whole architecture consists of fully convolutional layers, allowing for input images of different sizes. Furthermore, the skip connections intoduced in this work, are used to provide additional information that helps the decoder generate better semantic features. They can act as a shortcut connection that helps the indirect flow of gradients to the earlier layers without any degradation.</p>
<p>Lastly, two main differences in the implementation can be observed compared to the original UNet paper. Both of them are chosen in order to have the same submodule implementation compared to the next model and remove any doubt of differences because of them.</p>
<ul>
<li>batch normalisation is used before passing through the ReLU activation</li>
<li>valid padding is applied after up-sampling instead of cropping the skip connections, before concatenating</li>
</ul>
<p>The code for the building blocks of this model is attached below in case any parts remain unclear.</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">class</span> <span style="font-weight:bold">DoubleConv</span>(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> __init__(self, in_channels, out_channels):
</span></span><span style="display:flex;"><span>        super(DoubleConv, self).__init__()
</span></span><span style="display:flex;"><span>        self.double_conv = nn.Sequential(
</span></span><span style="display:flex;"><span>            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=0, bias=<span style="font-weight:bold">False</span>),     
</span></span><span style="display:flex;"><span>            nn.BatchNorm2d(out_channels),
</span></span><span style="display:flex;"><span>            nn.ReLU(inplace=<span style="font-weight:bold">True</span>),
</span></span><span style="display:flex;"><span>            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=0, bias=<span style="font-weight:bold">False</span>),
</span></span><span style="display:flex;"><span>            nn.BatchNorm2d(out_channels),
</span></span><span style="display:flex;"><span>            nn.ReLU(inplace=<span style="font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> forward(self, x):
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">return</span> self.double_conv(x)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">class</span> <span style="font-weight:bold">Down</span>(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> __init__(self, in_channels, out_channels):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>        self.maxpool_conv = nn.Sequential(
</span></span><span style="display:flex;"><span>            nn.MaxPool2d(kernel_size = 2, stride = 2),
</span></span><span style="display:flex;"><span>            DoubleConv(in_channels, out_channels)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> forward(self, x):
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">return</span> self.maxpool_conv(x)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-weight:bold">class</span> <span style="font-weight:bold">Up</span>(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> __init__(self, in_channels, out_channels, padding_type):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>        self.padding_type = padding_type
</span></span><span style="display:flex;"><span>        <span style="font-style:italic"># half of the number of channels comes from concatination</span>
</span></span><span style="display:flex;"><span>        self.up = nn.ConvTranspose2d(in_channels, in_channels // 2, kernel_size=2, stride=2)
</span></span><span style="display:flex;"><span>        self.conv = DoubleConv(in_channels, out_channels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="font-weight:bold">def</span> forward(self, x1, x2):
</span></span><span style="display:flex;"><span>        x1 = self.up(x1)
</span></span><span style="display:flex;"><span>        x2 = x2[:,:, (x2.size()[2]-x1.size()[2])//2:x2.size()[2]-(x2.size()[2]-x1.size()[2])//2, (x2.size()[3]-x1.size()[3])//2:x2.size()[3]-(x2.size()[3]-x1.size()[3])//2]
</span></span><span style="display:flex;"><span>        x = torch.cat([x2, x1], dim=1)
</span></span><span style="display:flex;"><span>        <span style="font-weight:bold">return</span> self.conv(x)
</span></span></code></pre></div><h3 id="unet3">
  UNet3+
  <a class="heading-link" href="#unet3">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>Considering that UNet3+ is an extension of UNet, the in depth explenation of the latter was necessary. On this proposed model the decoder and encoder blocks remain the same, however new skip connections were introduced. It made full use of the multi-scale features by introducing full-scale skip connections, which incorporate low-level details with high-level semantics from feature maps in full scales. Beyond that also deep supervision was introduced as well as a classification-guided module, to reduce over-segmentation on none-organ images by jointly training with an image-level classification. However the two last improvements will not be used in the results produced for this ablation study.</p>
<p>Note that if a reader would like to also get results using them, an implementation of the model with deep supervision and cgm can be found in <a href="https://https://github.com/markos-gkozntaris/Does-UNet3-Generalize">our repo</a>.</p>
<h4 id="full-scale-skip-connections">
  Full-scale skip connections
  <a class="heading-link" href="#full-scale-skip-connections">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h4>
<p>None of the previous UNet variants explored sufficient information from full scales, failing to explicitly learn the position and boundary of an organ. To remedy this defect each decoder layer in UNet 3+ incorporates both smaller-scale and same-scale feature maps from encoder and larger-scale feature maps from decoder, which capture fine-grained details
and coarse-grained semantics in full scales.</p>
<p>In contrast to UNet, a set of inter decoder-encoder skip connection delivers the low-level detailed information to an encoder stage, from all the current and higher encoder stages by applying non-overlapping max pooling. On the other, hand a chain of intra decoder skip connections transmits the high-level semantic segmentation from lower stage decoder stages by applying linear interpolation.</p>
<p>A figure is provided to help in visualizing it. Furthermore, the decoder 3 stage shown will be analyzed as a hands-on example.</p>
<img class="img-center" src="/assets/unet3.png" alt="UNet3Plus" />
<p>For the higher decoder stages (in this case stage 1 and 2), a maxpooling followed by a DoubleConv + BN + ReLU is applied. The maxpooling is based on the distance of the stage to the current one.</p>
<p>In general, we get <strong>\( maxpool(2^i) \)</strong>, where \(i\) is equal to <em>current decoder stage index</em> minus the <em>encoder index where the skip connection is coming from</em>. For example, for decoder 3 and a skip connection coming from encoder 1, we would have \(i = 3-1 = 2\), so \(maxpool(4)\).</p>
<p>On the other hand, the lower encoder stages are not used, but the high-level semantic information comes instead from the larger-scale decoder stages (here stage 4 and 5). In order to concatenate them, considering the difference in tensor size from the intermediate operations, contrary to previously when maxpooling was applied, no bilinear interpolation is used with a scale factor \(2^i\).</p>
<p>All this skip connections are concatenated with the direct connection from the same stage encoder, and are passed through the DoubleConv + BN + ReLU, in order to create the output of each decoder stage.</p>
<p>Same as before, a piece of code is provided for the creation of \(De_3\):</p>
<p>Class definition:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-style:italic">&#39;&#39;&#39;create X_DE^3&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>self.h1_PT_hd3 = nn.MaxPool2d(4, 4, ceil_mode=<span style="font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>self.h1_PT_de3_Conv_BN_ReLU = Conv_BN_ReLU(filters[0],self.CatChannels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self.h2_PT_hd3 = nn.MaxPool2d(2, 2, ceil_mode=<span style="font-weight:bold">True</span>)
</span></span><span style="display:flex;"><span>self.h2_PT_de3_Conv_BN_ReLU = Conv_BN_ReLU(filters[1],self.CatChannels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self.h3_PT_de3_Conv_BN_ReLU = Conv_BN_ReLU(filters[2],self.CatChannels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self.hd4_UT_hd3 = nn.Upsample(scale_factor=2, mode=<span style="font-style:italic">&#39;bilinear&#39;</span>) 
</span></span><span style="display:flex;"><span>self.h4_PT_de3_Conv_BN_ReLU = Conv_BN_ReLU(self.UpChannels,self.CatChannels)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self.hd5_UT_hd3 = nn.Upsample(scale_factor=4, mode=<span style="font-style:italic">&#39;bilinear&#39;</span>)
</span></span><span style="display:flex;"><span>self.h5_PT_de3_Conv_BN_ReLU = Conv_BN_ReLU(filters[4],self.CatChannels)  
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>self.de3_Conv_BN_ReLU = Conv_BN_ReLU(self.UpChannels,self.UpChannels)
</span></span></code></pre></div><p>Forward pass:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="font-style:italic">&#39;&#39;&#39;create X_DE^3&#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>h1_PT_hd3 = self.h1_PT_de3_Conv_BN_ReLU(self.h1_PT_hd3(x1))
</span></span><span style="display:flex;"><span>self.debug(<span style="font-style:italic">&#34;h1_PT_hd3 = &#34;</span>, h1_PT_hd3.size())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>h2_PT_hd3 = self.h2_PT_de3_Conv_BN_ReLU(self.h2_PT_hd3(x2))
</span></span><span style="display:flex;"><span>self.debug(<span style="font-style:italic">&#34;h2_PT_hd3 = &#34;</span>, h2_PT_hd3.size())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>h3_Cat_hd3 = self.h3_PT_de3_Conv_BN_ReLU(x3)
</span></span><span style="display:flex;"><span>self.debug(<span style="font-style:italic">&#34;h3_Cat_hd3 = &#34;</span>, h3_Cat_hd3.size())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>hd4_UT_hd3 = self.h4_PT_de3_Conv_BN_ReLU(self.hd4_UT_hd3(hde4))      
</span></span><span style="display:flex;"><span>self.debug(<span style="font-style:italic">&#34;hd4_UT_hd3 = &#34;</span>, hd4_UT_hd3.size())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>hd5_UT_hd3 = self.h5_PT_de3_Conv_BN_ReLU(self.hd5_UT_hd3(hde5)) 
</span></span><span style="display:flex;"><span>self.debug(<span style="font-style:italic">&#34;hd5_UT_hd3 = &#34;</span>, hd5_UT_hd3.size())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>hde3 = self.de3_Conv_BN_ReLU(torch.cat((h1_PT_hd3, h2_PT_hd3, h3_Cat_hd3, hd4_UT_hd3, hd5_UT_hd3), 1)) 
</span></span></code></pre></div><h2 id="training">
  Training
  <a class="heading-link" href="#training">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<h3 id="data-loading">
  Data Loading
  <a class="heading-link" href="#data-loading">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>We configured the data-loaders to extract slices from the volumes in triples. This process can be visualized as having a rolling window over a sequence of (adjacent) slices, specifically of size 3 (as this is the value recommended in [2]). These triples comprise the 3 channels of the input, so after collation with the other triples, the dimensionality of the input is \((batchSize \times 3 \times 256 \times 256)\).</p>
<h3 id="optimizer-and-loss">
  Optimizer and Loss
  <a class="heading-link" href="#optimizer-and-loss">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>Considering that in this work the main purpose is to exploit the usefulness of adding more skip connections (so passing features that might have been lost on different stages), no bells and whistles will be used for training.
Below the optimizer and loss function can be found which was the same on both cases:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>optimizer = Adam(net.parameters(), lr=5e-4)
</span></span><span style="display:flex;"><span>criterion = nn.BCEWithLogitsLoss()
</span></span></code></pre></div><h2 id="difficulties">
  Difficulties
  <a class="heading-link" href="#difficulties">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>The two main difficulties faced while implementing this project were:</p>
<ul>
<li>some technicalities regarding the data-loading part</li>
<li>concatenation of tensors of different sizes during the implementation of the UNet3+ model</li>
<li>relatively limited computational resources</li>
</ul>
<p>Regarding the data-loading, the infrastucture we worked on initially did not allow for persistence of some intermediate pre-processed form of the dataset. Hence, a caching layer was implemented to make the loading more efficient by lowering disk access for consecutive triples, as a way to mitigate this limitation.</p>
<p>As for the computational resources, training models with this many number of parameters and such dataset size, was quite a challenge, even for this number of epochs. Each epoch of UNet took about 45 minutes to train on our infrastructure, and UNet3+ about 3.5 hours using one NVIDIA P100 GPU, and 6 hours on one NVIDIA T4 GPU which we later had to switch to.</p>
<h2 id="results">
  Results
  <a class="heading-link" href="#results">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<h3 id="comparison-of-the-two-architectures">
  Comparison of the two architectures
  <a class="heading-link" href="#comparison-of-the-two-architectures">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h3>
<p>Concerning the sizes of the two models, the UNet3+ has less parameters, but the forward/backward pass size is greater, probably due to the concatenations happening on the decoder side of the network. The exact numbers are provided in the following table:</p>
<table>
<thead>
<tr>
<th></th>
<th>UNet</th>
<th>UNet3+</th>
</tr>
</thead>
<tbody>
<tr>
<td>Total parameters</td>
<td>31,037,633</td>
<td>26,967,809</td>
</tr>
<tr>
<td>Forward/backward pass size (MB)</td>
<td>879.64</td>
<td>3447.00</td>
</tr>
<tr>
<td>Params size (MB)</td>
<td>118.40</td>
<td>102.87</td>
</tr>
<tr>
<td>Estimated Total Size (MB)</td>
<td>998.79</td>
<td>3550.62</td>
</tr>
</tbody>
</table>
<p>The train and test loss curves for 11 epochs are shown below:</p>
<img class="img-center" src="/assets/losses.png" alt="Losses" />
<p>In addition, four examples of the results are provided corresponding to the following two cases:</p>
<ol>
<li>Three slices from three different volumes of the CT scans.</li>
<li>A case with no mask on the ground truth data,</li>
</ol>
<h4 id="unet-1">
  UNet
  <a class="heading-link" href="#unet-1">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h4>
<img class="img-center" src="/assets/2-unet.png" alt="1-unet" />
<img class="img-center" src="/assets/3-unet.png" alt="2-unet" />
<img class="img-center" src="/assets/4-unet.png" alt="3-unet" />
<img class="img-center" src="/assets/1-unet.png" alt="4-unet" />
<h4 id="unet3-1">
  UNet3+
  <a class="heading-link" href="#unet3-1">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h4>
<img class="img-center" src="/assets/2-unet3.png" alt="1-unet3" />
<img class="img-center" src="/assets/3-unet3.png" alt="2-unet3" />
<img class="img-center" src="/assets/4-unet3.png" alt="3-unet3" />
<img class="img-center" src="/assets/1-unet3.png" alt="4-unet3" />
<h2 id="conclusions">
  Conclusions
  <a class="heading-link" href="#conclusions">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>Based on the two models trained for 11 epochs, the UNet3+ proved to perform better on this dataset, as can be noticed by the visualised results. It seems visually that the Intersection-Over-Union score is also higher. As expected from the introduction of new skip connections, more fine details were preserved leading to a better segmentation result.</p>
<p>Furthermore, visually in all samples assessed, both networks did not return false negatives (i.e. when a mask was empty, so was the predicted mask).</p>
<p>This was a fun project for us to work on and we learned a lot.</p>
<h2 id="references">
  References
  <a class="heading-link" href="#references">
    <i class="fa fa-link" aria-hidden="true"></i>
  </a>
</h2>
<p>[1] Ronneberger, Olaf, Philipp Fischer, and Thomas Brox. &ldquo;U-net: Convolutional networks for biomedical image segmentation.&rdquo; International Conference on Medical image computing and computer-assisted intervention. Springer, Cham, 2015.</p>
<p>[2] Huang, Huimin, et al. &ldquo;Unet 3+: A full-scale connected unet for medical image segmentation.&rdquo; ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020.</p>
<p>[3] Bilic, Patrick, et al. &ldquo;The liver tumor segmentation benchmark (lits).&rdquo; arXiv preprint arXiv:1901.04056 (2019).</p>
<p>[4] Lin, Tsung-Yi, et al. &ldquo;Microsoft coco: Common objects in context.&rdquo; European conference on computer vision. Springer, Cham, 2014.</p>

      </div>


      <footer>
        


        
        
        
      </footer>
    </article>

    
  </section>

      </div>

      
  <footer class="footer">
    <section class="container">
      
      
        ©
        
          2018 -
        
        2024
        
      
      
      
    </section>
  </footer>


    </main>

    
      
      <script src="/js/coder.min.cb0c595e02234420f3ad3886bf4a9bd2874d0e1e78e090138a9ef158b35aaf17.js" integrity="sha256-ywxZXgIjRCDzrTiGv0qb0odNDh544JATip7xWLNarxc="></script>
    

    
      <script src="/js/custom.js"></script>
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
    
      <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.16.2/build/highlight.min.js"></script>
    

    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-122370931-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

    

    

    

    

    

    
  </body>

</html>
